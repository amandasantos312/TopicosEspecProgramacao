{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57b504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset: Iris (binário) ===\n",
      "Classes: 0 = virginica (negativa), 1 = versicolor (positiva)\n",
      "Features (4): ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Instâncias totais: 100 | Nº de features: 4\n",
      "\n",
      "=== 10 primeiras linhas ===\n",
      " sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)         classe\n",
      "               7.0               3.2                4.7               1.4 versicolor (1)\n",
      "               6.4               3.2                4.5               1.5 versicolor (1)\n",
      "               6.9               3.1                4.9               1.5 versicolor (1)\n",
      "               5.5               2.3                4.0               1.3 versicolor (1)\n",
      "               6.5               2.8                4.6               1.5 versicolor (1)\n",
      "               5.7               2.8                4.5               1.3 versicolor (1)\n",
      "               6.3               3.3                4.7               1.6 versicolor (1)\n",
      "               4.9               2.4                3.3               1.0 versicolor (1)\n",
      "               6.6               2.9                4.6               1.3 versicolor (1)\n",
      "               5.2               2.7                3.9               1.4 versicolor (1)\n",
      "\n",
      "Tamanhos -> treino: 70 | teste: 30\n",
      "\n",
      "=== Regressão Logística ===\n",
      "Acurácia: 0.967\n",
      "Precisão (positiva=versicolor): 1.000\n",
      "Recall   (positiva=versicolor): 0.933\n",
      "F1       (positiva=versicolor): 0.966\n",
      "\n",
      "Matriz de Confusão (linhas=Real, colunas=Previsto):\n",
      "           Prev 0  Prev 1\n",
      "Real 0  |    15       0   <- virginica (0)\n",
      "Real 1  |     1      14   <- versicolor (1)\n",
      "\n",
      "=== Predições Individuais — Regressão Logística ===\n",
      "Entrada [np.float64(5.8), np.float64(2.8), np.float64(4.6), np.float64(1.4)] -> prev=versicolor (1) | P(virginica)= 0.159 | P(versicolor)= 0.841\n",
      "Entrada [np.float64(6.3), np.float64(3.0), np.float64(5.8), np.float64(2.2)] -> prev=virginica (0) | P(virginica)= 0.992 | P(versicolor)= 0.008\n",
      "Entrada [np.float64(6.0), np.float64(2.9), np.float64(4.5), np.float64(1.5)] -> prev=versicolor (1) | P(virginica)= 0.161 | P(versicolor)= 0.839\n",
      "\n",
      "=== SVM (Linear) ===\n",
      "Acurácia: 0.967\n",
      "Precisão (positiva=versicolor): 1.000\n",
      "Recall   (positiva=versicolor): 0.933\n",
      "F1       (positiva=versicolor): 0.966\n",
      "\n",
      "Matriz de Confusão (linhas=Real, colunas=Previsto):\n",
      "           Prev 0  Prev 1\n",
      "Real 0  |    15       0   <- virginica (0)\n",
      "Real 1  |     1      14   <- versicolor (1)\n",
      "\n",
      "=== Predições Individuais — SVM (Linear) ===\n",
      "Entrada [np.float64(5.8), np.float64(2.8), np.float64(4.6), np.float64(1.4)] -> prev=versicolor (1) | P(virginica)= 0.201 | P(versicolor)= 0.799\n",
      "Entrada [np.float64(6.3), np.float64(3.0), np.float64(5.8), np.float64(2.2)] -> prev=virginica (0) | P(virginica)= 0.999 | P(versicolor)= 0.001\n",
      "Entrada [np.float64(6.0), np.float64(2.9), np.float64(4.5), np.float64(1.5)] -> prev=versicolor (1) | P(virginica)= 0.138 | P(versicolor)= 0.862\n",
      "\n",
      "=== Naive Bayes (Gaussiano) ===\n",
      "Acurácia: 0.933\n",
      "Precisão (positiva=versicolor): 1.000\n",
      "Recall   (positiva=versicolor): 0.867\n",
      "F1       (positiva=versicolor): 0.929\n",
      "\n",
      "Matriz de Confusão (linhas=Real, colunas=Previsto):\n",
      "           Prev 0  Prev 1\n",
      "Real 0  |    15       0   <- virginica (0)\n",
      "Real 1  |     2      13   <- versicolor (1)\n",
      "\n",
      "=== Predições Individuais — Naive Bayes (Gaussiano) ===\n",
      "Entrada [np.float64(5.8), np.float64(2.8), np.float64(4.6), np.float64(1.4)] -> prev=versicolor (1) | P(virginica)= 0.006 | P(versicolor)= 0.994\n",
      "Entrada [np.float64(6.3), np.float64(3.0), np.float64(5.8), np.float64(2.2)] -> prev=virginica (0) | P(virginica)= 1.000 | P(versicolor)= 0.000\n",
      "Entrada [np.float64(6.0), np.float64(2.9), np.float64(4.5), np.float64(1.5)] -> prev=versicolor (1) | P(virginica)= 0.017 | P(versicolor)= 0.983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Classificação Binária – Iris (todas as 4 features)\n",
    "# Versicolor (1, classe positiva) vs Virginica (0, classe negativa)\n",
    "# Modelos:\n",
    "#   1) Regressão Logística (fronteira linear + probabilidades)\n",
    "#   2) SVM (Linear)       (hiperplano linear; aqui com probability=True)\n",
    "#   3) Naive Bayes (Gaussiano) (assume distribuição normal e independência condicional)\n",
    "#\n",
    "# Saídas (por modelo):\n",
    "#   - métricas no TESTE: acurácia, precisão, recall, F1\n",
    "#   - matriz de confusão\n",
    "#   - predições individuais com probabilidades (quando disponíveis)\n",
    "# ===============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) Carregar Iris e tornar BINÁRIA: 1=versicolor (positiva), 0=virginica\n",
    "# ---------------------------------------------------------------\n",
    "iris = load_iris()\n",
    "X_full = iris.data            # (150, 4): sepal len, sepal wid, petal len, petal wid\n",
    "y_full = iris.target          # 0=setosa, 1=versicolor, 2=virginica\n",
    "\n",
    "mask = (y_full != 0)          # remove setosa\n",
    "X = X_full[mask]              # usaremos TODAS as 4 features\n",
    "y = (y_full[mask] == 1).astype(int)  # versicolor -> 1, virginica -> 0\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "print(\"=== Dataset: Iris (binário) ===\")\n",
    "print(\"Classes: 0 = virginica (negativa), 1 = versicolor (positiva)\")\n",
    "print(\"Features (4):\", list(feature_names))\n",
    "print(f\"Instâncias totais: {X.shape[0]} | Nº de features: {X.shape[1]}\\n\")\n",
    "\n",
    "# (Opcional) Mostrar as 10 primeiras linhas para contextualizar\n",
    "df_demo = pd.DataFrame(X, columns=feature_names)\n",
    "df_demo[\"classe\"] = np.where(y==1, \"versicolor (1)\", \"virginica (0)\")\n",
    "print(\"=== 10 primeiras linhas ===\")\n",
    "print(df_demo.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2) Split treino/teste (estratificado preserva a proporção das classes)\n",
    "# ---------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(f\"Tamanhos -> treino: {X_train.shape[0]} | teste: {X_test.shape[0]}\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3) Definir os 3 modelos (todos em Pipeline; padronização ajuda RL e SVM)\n",
    "# ---------------------------------------------------------------\n",
    "modelos = [\n",
    "    (\"Regressão Logística\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(random_state=RANDOM_STATE, max_iter=1000))\n",
    "    ])),\n",
    "    (\"SVM (Linear)\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # Usamos SVC linear com probability=True para ter predict_proba\n",
    "        (\"clf\", SVC(kernel=\"linear\", probability=True, random_state=RANDOM_STATE))\n",
    "    ])),\n",
    "    (\"Naive Bayes (Gaussiano)\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),   # opcional para NB; mantemos por padrão\n",
    "        (\"clf\", GaussianNB())\n",
    "    ])),\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4) Funções utilitárias\n",
    "# ---------------------------------------------------------------\n",
    "def avaliar_modelo(nome, modelo, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Treina o modelo, faz previsões no TESTE e imprime:\n",
    "    - Acurácia, Precisão (classe positiva=1), Recall (positiva=1), F1\n",
    "    - Matriz de confusão\n",
    "    Retorna o modelo treinado.\n",
    "    \"\"\"\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)   # linhas = Real, colunas = Previsto\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, pos_label=1)\n",
    "    rec  = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1   = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    print(f\"=== {nome} ===\")\n",
    "    print(f\"Acurácia: {acc:.3f}\")\n",
    "    print(f\"Precisão (positiva=versicolor): {prec:.3f}\")\n",
    "    print(f\"Recall   (positiva=versicolor): {rec:.3f}\")\n",
    "    print(f\"F1       (positiva=versicolor): {f1:.3f}\")\n",
    "    print(\"\\nMatriz de Confusão (linhas=Real, colunas=Previsto):\")\n",
    "    print(\"           Prev 0  Prev 1\")\n",
    "    print(f\"Real 0  |   {cm[0,0]:>3}     {cm[0,1]:>3}   <- virginica (0)\")\n",
    "    print(f\"Real 1  |   {cm[1,0]:>3}     {cm[1,1]:>3}   <- versicolor (1)\\n\")\n",
    "    return modelo\n",
    "\n",
    "def predicoes_individuais(nome, modelo, exemplos):\n",
    "    \"\"\"\n",
    "    Mostra predições individuais com probabilidades quando o modelo oferece.\n",
    "    \"\"\"\n",
    "    print(f\"=== Predições Individuais — {nome} ===\")\n",
    "    if hasattr(modelo, \"predict_proba\"):\n",
    "        probas = modelo.predict_proba(exemplos)   # colunas: [P(0), P(1)]\n",
    "        preds  = modelo.predict(exemplos)\n",
    "        for x, p, pred in zip(exemplos, probas, preds):\n",
    "            print(f\"Entrada {list(x)} -> prev={'versicolor (1)' if pred==1 else 'virginica (0)'} \"\n",
    "                  f\"| P(virginica)= {p[0]:.3f} | P(versicolor)= {p[1]:.3f}\")\n",
    "    else:\n",
    "        preds = modelo.predict(exemplos)\n",
    "        for x, pred in zip(exemplos, preds):\n",
    "            print(f\"Entrada {list(x)} -> prev={'versicolor (1)' if pred==1 else 'virginica (0)'} \"\n",
    "                  f\"(modelo não fornece probabilidade)\")\n",
    "    print()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5) Rodar os 3 modelos (treinar, avaliar, prever exemplos)\n",
    "# ---------------------------------------------------------------\n",
    "treinados = []\n",
    "for nome, mdl in modelos:\n",
    "    mfit = avaliar_modelo(nome, mdl, X_train, y_train, X_test, y_test)\n",
    "    treinados.append((nome, mfit))\n",
    "\n",
    "    # Três exemplos com TODAS as 4 features (ordem: sepal len, sepal wid, petal len, petal wid)\n",
    "    exemplos = np.array([\n",
    "        [5.8, 2.8, 4.6, 1.4],  # tende a versicolor\n",
    "        [6.3, 3.0, 5.8, 2.2],  # tende a virginica\n",
    "        [6.0, 2.9, 4.5, 1.5]   # ponto “fronteira”\n",
    "    ])\n",
    "    predicoes_individuais(nome, mfit, exemplos)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
